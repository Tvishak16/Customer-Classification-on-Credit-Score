{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3846912,"sourceType":"datasetVersion","datasetId":2289007}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Packages for EDA\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport string\nimport os\n\n# Data Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PowerTransformer\nfrom datasist.structdata import detect_outliers\nfrom sklearn.metrics import mean_squared_error\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.impute import SimpleImputer\nimport category_encoders as ce\nimport re\n\n# Outlier Transformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Visulalization\n%matplotlib inline\nmatplotlib.rc((\"xtick\", \"ytick\", \"text\"), c=\"k\")\nmatplotlib.rc(\"figure\", dpi=80)\n\n# Modeling and evaluation\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import (\n    BaggingClassifier,\n    ExtraTreesClassifier,\n    RandomForestClassifier,\n    StackingClassifier,\n    HistGradientBoostingClassifier,\n    AdaBoostClassifier\n)\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.inspection import permutation_importance\nimport joblib\n\n# Packages options\nsns.set(rc={'figure.figsize': [14, 7]}, font_scale=1.2) # Standard figure size for all\nnp.seterr(divide='ignore', invalid='ignore', over='ignore') ;\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"88209f56","outputId":"b380a2d1-25b7-454d-d030-f0441dbeea29"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"./train.csv\", low_memory=False)\ntrain_df[\"is_train\"] = True\ntest_df = pd.read_csv(\"./test.csv\", low_memory=False)\ntest_df[\"is_train\"] = False\n\ndf = pd.concat([train_df, test_df])","metadata":{"id":"675b4496"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{"id":"a82eedac"}},{"cell_type":"code","source":"df.columns","metadata":{"id":"9c532a6d","outputId":"a130dac0-c657-4d91-e624-13a14b037449"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping irrelevant columns\ndf.drop([\"Name\", \"SSN\", \"ID\"], axis=1, inplace=True, errors=\"ignore\")","metadata":{"id":"6a91b9e1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change dtype for specific columns\ncolumns_to_convert = [\"Month\", \"Occupation\", \"Type_of_Loan\", \"Credit_History_Age\", \"Payment_Behaviour\"]\ndf[columns_to_convert] = df[columns_to_convert].astype(\"category\")","metadata":{"id":"0684ad8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"5178377a","outputId":"c2bc20aa-eab3-4b43-af42-6ca91f222031"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"id":"5ca8cf68","outputId":"2eaa927d-5c31-48a7-8f40-8b59a5b6ad20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"id":"e9ba3923","outputId":"87111034-8440-4848-c253-4416e2d5a508"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"id":"53343788","outputId":"58b36338-34f5-4b7c-a93a-9d895b41caa2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(exclude=np.number).T","metadata":{"id":"91d5874f","outputId":"266d206a-f7de-405c-f96d-9f73feeff128"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get Unique Values\ndef get_unique_values(df):\n    cat_cols = df.select_dtypes(\"object\").columns\n\n    data_info = np.zeros((len(cat_cols), 5), dtype=\"object\")\n    for i, col in enumerate(cat_cols):\n        if len(df[col].unique()) > 5000:\n            continue\n        else:\n            unique_values, counts = np.unique(\n                np.array(df[col], dtype=str), return_counts=True)\n            num_of_uv = len(unique_values)\n            unique_val_percent = np.round(counts / counts.sum(), 2)\n            data_info[i, :] = [col, unique_values.tolist(\n            ), counts.tolist(), num_of_uv, unique_val_percent]\n    return pd.DataFrame(data_info, columns=[\"column\", \"unique\", \"counts\", \"len_unique_values\", \"%_unique_values\"])","metadata":{"id":"ab774739"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_values_df = get_unique_values(df)\nunique_values_df.head()","metadata":{"id":"5becd7a5","outputId":"d474f2fc-7b7c-41f3-ae4c-2e38700c7a46"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing","metadata":{"id":"6e1de2d3"}},{"cell_type":"code","source":"class DataProcessor:\n    def __init__(self, groupby, data_frame):\n        self.groupby = groupby\n        self.df = data_frame\n\n    def get_month(self, x):\n     if not pd.isnull(x):\n         year_month = re.findall(r\"\\d+\", x)\n         months = (int(year_month[0])*12) + np.int64(year_month[-1])\n         return months\n     else:\n         x\n\n    @staticmethod\n    def get_numbers(text):\n        digits = re.findall(r'\\d+', str(text))\n        digits = ','.join(digits)\n        return digits\n\n    @staticmethod\n    def replace_special_character(text):\n        if \"NM\" in str(text):\n            return \"No\"\n        if \"payments\" in str(text) or \"_\" not in str(text):\n            return text\n        clean_text = str(text).replace(\"_\", \"\")\n        return np.nan if clean_text == \"nan\" else clean_text\n\n    @staticmethod\n    def preprocess_text(texts:str) -> tuple[dict, list[list[str]]]:\n        dictionary = {}\n        tokens = [str(text).lower().replace(\"and\", \"\").split(\",\") for text in texts]\n        tokens = [[token.strip() for token in token_list if token not in string.punctuation] for token_list in tokens]\n        for token_list in tokens:\n            for token in token_list:\n                if token not in dictionary:\n                    size = len(dictionary)\n                    dictionary[token] = size\n        return (dictionary, [\"|\".join(words) for words in tokens])\n\n\n    @staticmethod\n    def fill_na(df: pd.DataFrame, groupby=None):\n        cat_features = df.select_dtypes(exclude=\"number\").columns.drop(\n            [\"Type_of_Loan\", \"is_train\"])\n        num_features = df.select_dtypes(include=\"number\").columns\n        df[\"Type_of_Loan\"].fillna(\"not specified\", inplace=True)\n        if \"Credit_Score\" in df.columns:\n            cat_features = cat_features.drop(\"Credit_Score\")\n\n        # Replacing Categorial Columns with Mode\n        def fill_na_cat(df):\n            df[cat_features] = df.groupby(groupby)[cat_features].transform(\n                lambda x: x.fillna(x.mode()[0]))\n            return df\n\n        # Replacing Numerical Columns with Median\n        def fill_na_num(df):\n            df[num_features] = df.groupby(groupby)[num_features].transform(\n                lambda x: x.mask(x < 0, np.nan).fillna(x.median()))\n            return df\n\n        df = fill_na_cat(df)\n        df = fill_na_num(df)\n        return df\n    def preprocess(self):\n        # Age\n        self.df['Age'] = self.df.Age.apply(DataProcessor.get_numbers)\n        # Handle Special Characters\n        self.df = self.df.applymap(DataProcessor.replace_special_character)\n        self.df = self.df.apply(lambda x: pd.to_numeric(x, errors=\"ignore\"))\n        # Credit Mix\n        self.df[\"Credit_Mix\"] = self.df.groupby(self.groupby)[\"Credit_Mix\"].transform(lambda x: x.replace(\"\", x.mode()[0]))\n        # Payment Behaviour\n        self.df[\"Payment_Behaviour\"] = self.df.groupby(self.groupby)[\"Payment_Behaviour\"].transform(\n            lambda x: x.replace(\"!@9#%8\", x.mode()[0])\n        )\n        self.df[\"Payment_Behaviour\"] = self.df[\"Payment_Behaviour\"].transform(\n            lambda x: x.replace(\"!@9#%8\", x.mode()[0])\n        )\n        # Type of Loan\n        self.df[\"Type_of_Loan\"] = self.df[[\"Type_of_Loan\"]].apply(lambda x:  DataProcessor.preprocess_text(x.values)[-1])\n        self.df[\"Type_of_Loan\"] = self.df[\"Type_of_Loan\"].str.replace(\" \", \"_\").str.replace(\"|\", \" \").replace(\"nan\", np.nan)\n        # Credit History Age\n        self.df[\"Credit_History_Age\"] = self.df[\"Credit_History_Age\"].apply(lambda x: self.get_month(x))\n        # Monthly Balance\n        self.df[\"Monthly_Balance\"] = pd.to_numeric(self.df.Monthly_Balance, errors=\"coerce\")\n        # Replacing account balances less than zero with zero\n        self.df.loc[self.df[\"Num_Bank_Accounts\"] < 0, \"Num_Bank_Accounts\"] = 0\n        # Replace \"nan\" values in the 'Type_of_Loan' column with NaN for consistency\n        self.df.loc[self.df[\"Type_of_Loan\"] == \"nan\", \"Type_of_Loan\"] = np.nan\n        # Replace \"nan\" values in the 'Occupation' column with NaN for consistency\n        self.df.loc[self.df[\"Occupation\"] == \"\", \"Occupation\"] = np.nan\n        self.df.loc[self.df[\"Occupation\"] == \"_______\", \"Occupation\"] = np.nan\n        # Replace \"nan\" values in the 'Credit_Mix' column with NaN for consistency\n        self.df.loc[self.df[\"Credit_Mix\"] == \"\", \"Credit_Mix\"] = np.nan\n\n        # Negetive Numbers\n        self.df['Num_of_Delayed_Payment'] = pd.to_numeric(self.df['Num_of_Delayed_Payment'], errors='coerce')\n\n        self.df.loc[self.df['Num_of_Delayed_Payment'] < 0, 'Num_of_Delayed_Payment'] = np.nan\n        self.df.loc[self.df['Delay_from_due_date'] < 0, 'Delay_from_due_date'] = np.nan\n\n        # Filling missing values\n        self.df = DataProcessor.fill_na(self.df, \"Customer_ID\")\n\n        return self.df","metadata":{"id":"9c3328c8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocesor = DataProcessor(\"Customer_ID\", df)\nnew_df = preprocesor.preprocess()","metadata":{"id":"f4c62210"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.isna().sum()","metadata":{"id":"ebb8e4c2","outputId":"669d48fa-345b-477c-d831-626de58a033c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df[new_df.isna().any(axis=1)]","metadata":{"id":"4e6aaa89","outputId":"19f9b5e4-c36e-46d7-f194-613cfc9457ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClipOutliersTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self,\n                 lower_quantile,\n                 upper_quantile,\n                 multiply_by=1.5,\n                 replace_with_median: bool = False):\n        self.lower_quantile = lower_quantile\n        self.upper_quantile = upper_quantile\n        self.multiply_by = multiply_by\n        self.replace_with_median = replace_with_median\n\n        self.lower_limit = 0\n        self.upper_limit = 0\n        self.feature_names_in_ = None\n\n    def fit(self, X, y=None):\n        q1, q3 = np.quantile(X, [self.lower_quantile, self.upper_quantile])\n        iqr = q3 - q1\n        self.lower_limit = q1 - (self.multiply_by * iqr)\n        self.upper_limit = q3 + (self.multiply_by * iqr)\n        return self\n\n    def transform(self, X):\n        if self.replace_with_median:\n            return np.where(\n                ((X >= self.lower_limit) & (X <= self.upper_limit)), X,\n                np.median(X))\n        else:\n            return np.clip(X, self.lower_limit, self.upper_limit)\n\n\ndef get_skewness(df, lower=None, upper=None):\n    columns = df.columns\n    skewness: pd.Series = df[columns].skew()\n    highly_skewed = skewness[(skewness <= lower) |\n                             (skewness >= upper)].index.to_list()\n    lowly_skewed = skewness[(skewness > lower)\n                            & (skewness < upper)].index.to_list()\n    return (highly_skewed, lowly_skewed)\n\n\ndef remove_outliers(df: pd.DataFrame):\n    category = df.select_dtypes(exclude=\"number\").columns.drop(\n        [\"Credit_Score\", \"is_train\"])\n    numbers = df.select_dtypes(include=\"number\").columns\n\n    highly_skewed, lowly_skewed = get_skewness(df[numbers],\n                                               lower=-0.8,\n                                               upper=0.8)\n\n    df[highly_skewed] = df[highly_skewed].apply(\n        lambda x: ClipOutliersTransformer(\n            0.25, 0.75, multiply_by=1.5, replace_with_median=True).\n        fit_transform(x))\n\n    df[lowly_skewed] = df[lowly_skewed].apply(\n        lambda x: ClipOutliersTransformer(\n            0.25, 0.75, multiply_by=1.5, replace_with_median=False).\n        fit_transform(x))\n    return df","metadata":{"id":"467e73f2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = remove_outliers(new_df)","metadata":{"id":"8a9b65a1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.to_csv(\"new.csv\", index=False)","metadata":{"id":"5cc2fd2e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization","metadata":{"id":"ae90cd90"}},{"cell_type":"code","source":"# Visualization Super Class\ndef make_boxplot(df, column, ax):\n    sns.boxplot(x=\"Credit_Score\", y=column, data=df, ax=ax, width=0.8, palette=\"Set2\")\n    plt.xticks(rotation=90)\n    # add the five number summary to the plot\n    plt.title(column, fontdict={\"fontsize\": 15})\n    plt.xticks(rotation=0)\n\ndef plot_boxplot_num_cols(df):\n    fig = plt.figure(figsize=(18, 18), dpi=300)\n    numb_columns = df.select_dtypes(include=\"number\").columns\n    for column in numb_columns:\n        ax = fig.add_subplot(5, 4, list(numb_columns).index(column)+1)\n        make_boxplot(df, column, ax)\n        plt.tight_layout(pad=0.3)\n    plt.tight_layout()\n    plt.show()\n\n# Define function to create histograms for all numeric features\ndef plot_histograms(df):\n    numeric_cols = df.select_dtypes(include=np.number).columns\n    num_cols = len(numeric_cols)\n    num_plots_per_row = 3\n    num_rows = (num_cols // num_plots_per_row) + (num_cols % num_plots_per_row > 0)\n\n    fig, axes = plt.subplots(num_rows, num_plots_per_row, figsize=(15, 10))\n    axes = axes.ravel()\n\n    for i, col in enumerate(numeric_cols):\n        sns.histplot(df[col], bins=20, kde=True, ax=axes[i])\n        axes[i].set_title(f'Histogram of {col}')\n        axes[i].set_xlabel(col)\n        axes[i].set_ylabel('Frequency')\n\n        if i >= num_cols - 1:\n            for j in range(i + 1, len(axes)):\n                axes[j].remove()\n            break\n\n    plt.tight_layout()\n    plt.show()\n\n# Define function to create Distribution plot for all numeric features\ndef plot_distribution_plots(df):\n    numeric_cols = df.select_dtypes(include=np.number).columns\n    num_cols = len(numeric_cols)\n    num_plots_per_row = 3\n    num_rows = (num_cols // num_plots_per_row) + (num_cols % num_plots_per_row > 0)\n\n    fig, axes = plt.subplots(num_rows, num_plots_per_row, figsize=(15, 10))\n    axes = axes.ravel()\n\n    for i, col in enumerate(numeric_cols):\n        sns.distplot(df[col], bins=20, kde=True, ax=axes[i])\n        axes[i].set_title(f'Distribution of {col}')\n        axes[i].set_xlabel(col)\n        axes[i].set_ylabel('Density')\n\n        if i >= num_cols - 1:\n            for j in range(i + 1, len(axes)):\n                axes[j].remove()\n            break\n\n    plt.tight_layout()\n    plt.show()\n\ndef make_countplot(df: pd.DataFrame):\n    cat_cols = df.select_dtypes(exclude=\"number\").columns.drop(\n        ['Credit_Score','Customer_ID', \"Type_of_Loan\"])\n    cat_cols = list(cat_cols)\n    cat_cols.pop(-1)\n    cat_cols.insert(-2, \"Payment_Behaviour\")\n\n    fig, axes = plt.subplots(figsize=(12, 12), dpi=300)\n    fig.suptitle(\"Counts of categorical columns\")\n    axes.grid(visible=False)\n    axes.xaxis.set_tick_params(labelbottom=False)\n    axes.yaxis.set_tick_params(labelleft=False)\n\n    def __plot_graph(df, col, ax: plt.Axes, legend=False):\n        sns.countplot(\n            data=df,\n            x=col,\n            ax=ax,\n            hue=\"Credit_Score\",\n        )\n        # label =ax.get_xlabel()\n        ax.set_xlabel(col, fontdict={\"size\": 9})\n        ax.set_title(f\"by {col}\", fontdict={\"size\": 9})\n        ax.get_xticklabels()\n        ax.tick_params(labelsize=7, axis=\"y\")\n        ax.set_xticklabels(ax.get_xticklabels(),\n                           rotation=90,\n                           fontdict=dict(size=7))\n        ax.grid(False)\n        if legend:\n            ax.legend(shadow=True,\n                      loc=\"best\",\n                      facecolor=\"inherit\",\n                      frameon=True)\n        else:\n            ax.legend_ = None\n        plt.tight_layout(w_pad=1)\n\n    for i, col in enumerate(cat_cols, 1):\n        if i == 3:\n            continue\n        ax = fig.add_subplot(2, 3, i)\n        __plot_graph(df, col=col, ax=ax)\n\n    ax2 = fig.add_axes((0.74, 0.527, 0.23, 0.35))\n    __plot_graph(df, col=\"Payment_Behaviour\", ax=ax2, legend=True)\n    plt.show(True)\n\ndef plot_correlation_matrix(data):\n    corr = data.corr(numeric_only=True)\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    fig = plt.figure(figsize=(10, 10), dpi=150)\n\n    sns.heatmap(corr, annot=True, mask=mask, fmt=\".0%\", annot_kws={\"size\":10})\n    plt.grid(False)\n    plt.tick_params(axis=\"both\", labelsize=5)\n    plt.tight_layout()\n    plt.title(\"Correlation Matrix\")\n    plt.show()","metadata":{"id":"64c520a2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_histograms(new_df)","metadata":{"id":"d59b1571","outputId":"64a4f625-8571-4f4e-d63e-a881b373e820"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_boxplot_num_cols(new_df)","metadata":{"id":"210f345e","outputId":"e47e63ae-47e6-4304-d411-befa3f6fb765"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution_plots(new_df)","metadata":{"id":"cfbcabfc","outputId":"6e73db29-0201-4796-f892-112f906b976b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_countplot(new_df)","metadata":{"id":"d9379bef","outputId":"c44e693a-cd68-4c6e-aadc-e15c0d1a0672"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_correlation_matrix(new_df)","metadata":{"id":"d12532d6","outputId":"eec17f0c-f4f3-428e-8365-f80f0c6e27bf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{"id":"f9f725a0"}},{"cell_type":"code","source":"def feature_engineering(df):\n    # Define mappings for categorical variables\n\n    credit_score_mapping = {\n        \"Poor\": 0,\n        \"Standard\": 1,\n        \"Good\": 2\n    }\n\n    credit_mix_mapping = {\n        \"Bad\": 0,\n        \"Standard\": 1,\n        \"Good\": 2\n    }\n\n    min_amount_mapping = {\n        \"Yes\": 1,\n        \"No\": 0\n    }\n\n    # Replace categorical variables with mapped values\n\n    df['Credit_Score'].replace(credit_score_mapping, inplace=True)\n    df['Credit_Mix'].replace(credit_mix_mapping, inplace=True)\n    df['Payment_of_Min_Amount'].replace(min_amount_mapping, inplace=True)\n\n    # Perform one-hot encoding for selected categorical variables\n\n    df = pd.get_dummies(df, columns=['Occupation', 'Payment_Behaviour'])\n\n    # Drop unnecessary columns\n\n    df = df.drop(['Customer_ID', 'Month', 'Type_of_Loan', 'is_train'], axis=1)\n\n    return df","metadata":{"id":"9979f1e0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = new_df[new_df[\"is_train\"]]","metadata":{"id":"16b078cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = feature_engineering(df)","metadata":{"id":"f68cb701"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"02cc1d78","outputId":"644edafa-ea44-43dc-a269-08f7aa72824b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling and Evaluation","metadata":{"id":"b6d9b385"}},{"cell_type":"code","source":"X, y = df.drop(\"Credit_Score\",axis=1).values , df[\"Credit_Score\"]","metadata":{"id":"9104cc81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts(normalize=True)","metadata":{"id":"809b62c4","outputId":"ca28e391-915f-4430-e670-495acdcafe2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rus = SMOTE(sampling_strategy='auto')\nX_data_rus, y_data_rus = rus.fit_resample(X, y)","metadata":{"id":"5da5972b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_data_rus.value_counts(normalize=True)","metadata":{"id":"173308df","outputId":"8a20b405-2325-4ed4-fbe4-9e1268e6ea72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_data_rus, y_data_rus, test_size=0.3, random_state=42,stratify=y_data_rus)","metadata":{"id":"823cfe9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scalar = PowerTransformer(method='yeo-johnson', standardize=True).fit(X_train)","metadata":{"id":"8546fcf3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = scalar.transform(X_train)\nX_test = scalar.transform(X_test)","metadata":{"id":"98d67914"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the models\nmodels = {\n    \"Bagging\": BaggingClassifier(n_jobs=-1),\n    \"ExtraTrees\": ExtraTreesClassifier(max_depth=10, n_jobs=-1),\n    \"RandomForest\": RandomForestClassifier(n_jobs=-1),\n    \"HistGradientBoosting\": HistGradientBoostingClassifier(),\n    \"XGB\": XGBClassifier(n_jobs=-1),\n    \"KNN\": KNeighborsClassifier(),\n    \"AdaBoost\": AdaBoostClassifier()\n}","metadata":{"id":"9f94c309"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize dictionaries to store scores\nprecision_scores = {}\nrecall_scores = {}\nf1_scores = {}\n\n# Iterate over each model\nfor model_name, model in models.items():\n    # Fit the model on the training data\n    model.fit(X_train, y_train)\n\n    # Predict on the test data\n    y_pred = model.predict(X_test)\n\n    # Generate classification report\n    report = classification_report(y_test, y_pred, output_dict=True)\n    report_df = pd.DataFrame(report)\n\n    # Store scores in dictionaries\n    precision_scores[model_name] = report['weighted avg']['precision']\n    recall_scores[model_name] = report['weighted avg']['recall']\n    f1_scores[model_name] = report['weighted avg']['f1-score']\n\n    # Generate confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Print the classification report for the model\n    print(f\"{model_name} Classification Report:\")\n    print(report_df)\n\n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=True, yticklabels=True)\n    plt.xlabel('Predicted labels')\n    plt.ylabel('True labels')\n    plt.title(f'{model_name} Confusion Matrix')\n    plt.show()\n","metadata":{"id":"e664c174","outputId":"b606ce60-ba03-4e7c-9f62-dc8b8b297d79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing Models\nplt.figure(figsize=(12, 6))\n\n# Precision Scores\nplt.subplot(1, 3, 1)\nsns.barplot(x=list(precision_scores.keys()), y=list(precision_scores.values()))\nplt.title(\"Precision Scores\")\nplt.xticks(rotation=90)\n\n# Recall Scores\nplt.subplot(1, 3, 2)\nsns.barplot(x=list(recall_scores.keys()), y=list(recall_scores.values()))\nplt.title(\"Recall Scores\")\nplt.xticks(rotation=90)\n\n# F1 Scores\nplt.subplot(1, 3, 3)\nsns.barplot(x=list(f1_scores.keys()), y=list(f1_scores.values()))\nplt.title(\"F1 Scores\")\nplt.xticks(rotation=90)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"16b83b80","outputId":"b1dfb30b-6b76-4125-d6db-aa9eb6be2155"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## StackingClassifier","metadata":{"id":"449c6c21"}},{"cell_type":"code","source":"model = StackingClassifier(list(models.items()), n_jobs=-1)","metadata":{"id":"34ed333b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train)","metadata":{"id":"027afcc6","outputId":"702ff89f-d807-48ae-b18b-657d2d5b3078"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train Score: \",model.score(X_train, y_train))","metadata":{"id":"07e8eb32","outputId":"4637adc7-52e0-499d-8644-62d27e737b77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Test Score: \",model.score(X_test, y_test))","metadata":{"id":"706ba3b8","outputId":"889fd8c5-6769-40af-b594-97afebb8a584"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)","metadata":{"id":"f392c634"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_pred,y_test))","metadata":{"id":"2f666473","outputId":"0a07bb55-d66a-4a64-aab6-b1bdaaba2013"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    # Generate confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=True, yticklabels=True)\n    plt.xlabel('Predicted labels')\n    plt.ylabel('True labels')\n    plt.title(f'Final Model Confusion Matrix')\n    plt.show()","metadata":{"id":"6ea136fa","outputId":"735616e4-e8f6-493d-d949-193cde2780a2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Importance","metadata":{"id":"c5ba11f2"}},{"cell_type":"code","source":"# Permutation Importances for HistGradientBoostingClassifier\nmodels[\"HistGradientBoosting\"].fit(X_train, y_train)\nresult = permutation_importance(models[\"HistGradientBoosting\"], X_test, y_test, n_repeats=10, random_state=42)","metadata":{"id":"a66f62fb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get feature importances\nhist_gb_feature_importance = result.importances_mean","metadata":{"id":"494c7512"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature names\nfeature_names = df.drop(\"Credit_Score\", axis=1).columns\n\n# Sort feature importances and feature names together\nsorted_indices = np.argsort(hist_gb_feature_importance)\nsorted_feature_names = feature_names[sorted_indices]\nsorted_importances = hist_gb_feature_importance[sorted_indices]\n\n# Choose colors for the bars\ncolors = plt.cm.viridis(np.linspace(0, 1, len(sorted_feature_names)))\n\n# Plot permutation importances with colors and sorted order\nplt.figure(figsize=(10, 15))\nplt.barh(sorted_feature_names, sorted_importances, color=colors)\nplt.xlabel('Mean Permutation Importance')\nplt.ylabel('Feature')\nplt.title('Importances of Features')\nplt.show()","metadata":{"id":"4c00d6f9","outputId":"763eb196-ed19-4945-c44f-059209dfdfea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_tab = pd.crosstab(values=df[\"Monthly_Balance\"], index=[\n                        df[\"Credit_Score\"], df[\"Credit_Mix\"]], columns=\"Monthly_Balance\", aggfunc=\"mean\").reset_index()\n\nmain_group = pd.pivot_table(cross_tab, \"Monthly_Balance\", \"Credit_Score\", aggfunc=np.mean)\ncross_tab","metadata":{"id":"f31b0a5d","outputId":"0db2e6ea-9c3f-4ff5-d944-7da5f85b94d9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = plt.cm.Accent\nb = plt.cm.Blues\n\nfig, ax = plt.subplots(figsize=(6, 4))\nfig.suptitle(\"Distribution of Monthly_Balance by Credit Score & Credit Mix\",\n             fontsize=11,\n             color=\"k\")\nfig.set_frameon(True)\n\npie1, *_, texts = ax.pie(x=main_group[\"Monthly_Balance\"],\n                         labels=main_group.index,\n                         autopct=\"%.1f%%\",\n                         radius=1.3,\n                         colors=[a(120, 1), b(100, 1),\n                                 a(0, 1)],\n                         pctdistance=0.8,\n                         textprops={\"size\": 9},\n                         frame=True)\nplt.setp(pie1, width=0.5)\nax.set_frame_on(True)\n\npie2, *_, texts = ax.pie(x=cross_tab[\"Monthly_Balance\"],\n                         autopct=\"%.0f%%\",\n                         radius=0.8,\n                         colors=[\n                             a(80, 0.9),\n                             a(80, 0.8),\n                             a(80, 0.7),\n                             b(100, 0.9),\n                             b(100, 0.8),\n                             b(100, 0.7),\n                             a(0, 0.8),\n                             a(0, 0.65),\n                             a(0, 0.5)\n],\n    textprops={\"size\": 8})\nplt.setp(pie2, width=0.5)\nlegend_labels = np.unique(cross_tab[\"Credit_Mix\"])\n\nlegend_handles = [\n    plt.plot([], label=legend_labels[0], c=\"k\"),\n    plt.plot([], label=legend_labels[1], c='b'),\n    plt.plot([], label=legend_labels[-1], c=\"g\")\n]\nplt.legend(shadow=True,\n           frameon=True,\n           facecolor=\"inherit\",\n           loc=\"best\",\n           title=\"credit Score & Mix\",\n           bbox_to_anchor=(1, 1, 0.5, 0.1))\n\nplt.show()","metadata":{"id":"36a642c1","outputId":"d9dd69f7-446e-42d9-a192-3345aeebe72c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Test Data","metadata":{"id":"d44699b6"}},{"cell_type":"code","source":"df_test = new_df[~new_df[\"is_train\"]]","metadata":{"id":"9f7c7807"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = feature_engineering(df_test)","metadata":{"id":"baa4ef41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.drop([\"Credit_Score\"], axis=1, inplace=True, errors=\"ignore\")","metadata":{"id":"0648d3ac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"id":"6057314e","outputId":"715a338c-9de7-4c9b-82ac-000985119c15"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform the test data using the same scalar used for training\nX_test_processed = scalar.transform(df_test.values)","metadata":{"id":"63298272"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict Credit_Score for the test data\ny_pred_test = model.predict(X_test_processed)","metadata":{"id":"52636b65"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the predicted Credit_Score to the test dataframe\ndf_test['Predicted_Credit_Score'] = y_pred_test","metadata":{"id":"e8941669"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head(10)","metadata":{"id":"010d5d4f","outputId":"a46dd763-b4bc-4b3c-d7ce-3d43dac3a725"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.to_csv('df_test_predicted.csv', index=False)","metadata":{"id":"7d47cc8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"a4c9c11c"},"execution_count":null,"outputs":[]}]}